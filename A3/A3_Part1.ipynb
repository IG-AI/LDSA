{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.87:7077\") \\\n",
    "        .appName(\"Daniel_Agstrand_A3_Part1\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",4)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q A.1.1\n",
    "\n",
    "rdd_en = spark_context.newAPIHadoopFile(\n",
    "    'hdfs://192.168.2.87:9000/europarl/europarl-v7.sv-en.en',\n",
    "    'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'org.apache.hadoop.io.Text'\n",
    ")\\\n",
    ".cache() # Keep this RDD in memory!\n",
    "\n",
    "line_count_en = rdd_en.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q A.1.2\n",
    "\n",
    "rdd_sv = spark_context.newAPIHadoopFile(\n",
    "    'hdfs://192.168.2.87:9000/europarl/europarl-v7.sv-en.sv',\n",
    "    'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'org.apache.hadoop.io.Text'\n",
    ")\\\n",
    ".cache() # Keep this RDD in memory!\n",
    "\n",
    "line_count_sv = rdd_sv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q A.1.3\n",
    "\n",
    "if line_count_sv == line_count_en:\n",
    "    print (\"Equal amount of lines! Number of lines: {}\".format(line_count_en))\n",
    "else:\n",
    "    print (\"Unequal amount of lines! Number of lines in en-sv = {} and Number of lines in sv-en = {}\"\\\n",
    "           .format(line_count_en, line_count_sv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q A.1.4\n",
    "\n",
    "rdd_en_partitions_count = rdd_en.getNumPartitions()\n",
    "rdd_sv_partitions_count = rdd_sv.getNumPartitions()\n",
    "\n",
    "total_partitions_count = rdd_en_partitions_count + rdd_sv_partitions_count\n",
    "\n",
    "print(\"Number of partions in rdd_en: {}\\nNumber of partions in rdd_sv: {}\\nNumber of total partions in all rdd: {}\"\\\n",
    "      .format(rdd_en_partitions_count, rdd_sv_partitions_count, total_partitions_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q A.2.1\n",
    "\n",
    "def lowercase_split(line):\n",
    "    line = str(line).lower()\n",
    "    line = str(line).split(\" \")\n",
    "    return line\n",
    "\n",
    "rdd_en_mapped = rdd_en.flatMap(lowercase_split) \n",
    "rdd_sv_mapped = rdd_sv.flatMap(lowercase_split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
